{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKmzs3RIUheC",
        "outputId": "67f9d70c-dfe5-476a-a2da-90423527f371"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qXoV7lJwV8i9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_csv = pd.read_parquet('/content/drive/MyDrive/train_processed.parquet')\n",
        "test_csv = pd.read_parquet('/content/drive/MyDrive/test_processed.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install category_encoders\n",
        "!pip install feature_engine"
      ],
      "metadata": {
        "id": "pLP7SwYfXciZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cdb4baa-2965-4ea0-c9c8-a0312f45460e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.0.5-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.4/722.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.0.0-py3-none-any.whl (728 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.7.1)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.0.5 torchmetrics-1.0.0\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.1-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.13.5)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.1\n",
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.6.1-py2.py3-none-any.whl (326 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (0.13.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->feature_engine) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature_engine) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.16.0)\n",
            "Installing collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras tokenizer\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence # for import pad_sequences\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks import ModelSummary, LearningRateMonitor\n",
        "\n",
        "from sklearn import model_selection\n",
        "import joblib\n",
        "\n",
        "\n",
        "import socket\n",
        "import re\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "if is_cuda:\n",
        "    print(is_cuda)\n",
        "    print(torch.cuda.current_device())\n",
        "    print(torch.cuda.device_count())\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "print('Using device:', device)\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "\n",
        "import category_encoders as ce\n",
        "import feature_engine.encoding as fe\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import sklearn.metrics as metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raEMBG-_WYnu",
        "outputId": "97809c83-f260-4bd9-c963-5e734eb1bddf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n",
            "1\n",
            "Tesla T4\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_numerical = ['is_title_missing' , 'is_description_missing' , 'is_bullet_missing' , 'Desc_len' , 'title_len' , 'Bullet_len']\n",
        "\n",
        "encoder_numerical = Pipeline([\n",
        "    (\"selector\", ColumnTransformer([(\"selector\", \"passthrough\", columns_numerical)], remainder=\"drop\")),\n",
        "    (\"normalizer\", StandardScaler())\n",
        "])\n",
        "\n",
        "catgorical_columns = ['PRODUCT_TYPE_ID']\n",
        "\n",
        "columns_text = ['TITLE' , 'DESCRIPTION' , 'BULLET_POINTS']\n",
        "\n",
        "for num in columns_numerical:\n",
        "    print(num)\n",
        "    if train_csv[num].dtype != np.float32:\n",
        "        print(f\"converting {num} to float32\")\n",
        "        train_csv[num] = train_csv[num].astype(np.float32)"
      ],
      "metadata": {
        "id": "dBkRBOfUYbiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a497425-c034-4c70-ae06-d0a00fa3e2ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_title_missing\n",
            "converting is_title_missing to float32\n",
            "is_description_missing\n",
            "converting is_description_missing to float32\n",
            "is_bullet_missing\n",
            "converting is_bullet_missing to float32\n",
            "Desc_len\n",
            "converting Desc_len to float32\n",
            "title_len\n",
            "converting title_len to float32\n",
            "Bullet_len\n",
            "converting Bullet_len to float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv['PRODUCT_TYPE_ID'] = train_csv['PRODUCT_TYPE_ID'].astype('category')\n",
        "test_csv['PRODUCT_TYPE_ID'] = test_csv['PRODUCT_TYPE_ID'].astype('category')"
      ],
      "metadata": {
        "id": "S09gXOoyhiAD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv['length_log'] = np.log1p(train_csv['PRODUCT_LENGTH'])"
      ],
      "metadata": {
        "id": "c5iVgJODutJy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train , Val_Test = model_selection.train_test_split(train_csv , test_size = 0.2 , random_state = 42)\n",
        "Validation , Test = model_selection.train_test_split(Train , test_size = 0.2 , random_state = 42)"
      ],
      "metadata": {
        "id": "1va7lmo-ZSvA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'length_log'\n",
        "\n",
        "X_train = Train\n",
        "Y_train = Train[target]\n",
        "\n",
        "X_Validation = Validation\n",
        "Y_Validation = Validation[target]\n",
        "\n",
        "X_Test = Test\n",
        "Y_Test = Test[target]"
      ],
      "metadata": {
        "id": "DXfiBhGOZ-fe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dataset"
      ],
      "metadata": {
        "id": "QTe_roEaatyP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'length_log'\n",
        "dd_train, dd_validation = dataset.build_pytorch_dataset(X_train,\n",
        "                                         X_Validation,\n",
        "                                         encoder_numerical = encoder_numerical,\n",
        "                                         categorical_names = catgorical_columns,\n",
        "                                         text_names = columns_text,\n",
        "                                         char_names = columns_text,\n",
        "                                         encoder_target = PowerTransformer(method = \"box-cox\"),\n",
        "                                         target_name = target,\n",
        "                                         verbose = True)\n",
        "\n",
        "dd_test = dataset.build_test_dataset(dd_train , X_Test , verbose = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kiA-ptKdCio",
        "outputId": "b2a0d8e6-e903-4fcd-a36e-0c89b899554b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target: length_log\n",
            "train: (1799758, 13)\n",
            "test: (1439806, 13)\n",
            "train set mode\n",
            "=> target encoding\n",
            "=> numerical encoding\n",
            "=> categorical encoding\n",
            "=> tokenizing TITLE\n",
            "==> TITLE vocabulary size 768429 \n",
            "=> tokenizing DESCRIPTION\n",
            "==> DESCRIPTION vocabulary size 578497 \n",
            "=> tokenizing BULLET_POINTS\n",
            "==> BULLET_POINTS vocabulary size 473100 \n",
            "=> tokenizing chars TITLE\n",
            "==> TITLE vocabulary size 2333 \n",
            "=> tokenizing chars DESCRIPTION\n",
            "==> DESCRIPTION vocabulary size 4886 \n",
            "=> tokenizing chars BULLET_POINTS\n",
            "==> BULLET_POINTS vocabulary size 3154 \n",
            "target min, max range (-4.826471725694244, 20.179163219126)\n",
            "test set mode\n",
            "=> target encoding\n",
            "=> numerical encoding\n",
            "=> categorical encoding\n",
            "TITLE vocabulary size 768429\n",
            "DESCRIPTION vocabulary size 578497\n",
            "BULLET_POINTS vocabulary size 473100\n",
            "TITLE vocabulary size 2333\n",
            "DESCRIPTION vocabulary size 4886\n",
            "BULLET_POINTS vocabulary size 3154\n",
            "target min, max range (-4.826471725694244, 17.99748060663224)\n",
            "target: length_log\n",
            "train: 1799758\n",
            "test: (359952, 13)\n",
            "test set mode\n",
            "=> target encoding\n",
            "=> numerical encoding\n",
            "=> categorical encoding\n",
            "TITLE vocabulary size 768429\n",
            "DESCRIPTION vocabulary size 578497\n",
            "BULLET_POINTS vocabulary size 473100\n",
            "TITLE vocabulary size 2333\n",
            "DESCRIPTION vocabulary size 4886\n",
            "BULLET_POINTS vocabulary size 3154\n",
            "target min, max range (-4.826471725694244, 20.179163219126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train , X_Test , X_Validation\n",
        "del Train , Val_Test\n",
        "del Validation , Test\n",
        "del train_csv"
      ],
      "metadata": {
        "id": "-muD-LEednMc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import model"
      ],
      "metadata": {
        "id": "QGfgMRJ6wk3A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 300\n",
        "train_loader = DataLoader(dd_train, shuffle = True, batch_size = batch_size, collate_fn = dataset.pytorch_collate_fn)\n",
        "validation_loader = DataLoader(dd_validation, shuffle = False, batch_size = batch_size, collate_fn = dataset.pytorch_collate_fn)"
      ],
      "metadata": {
        "id": "v-pQ9lU4vlZH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dd_train , dd_validation"
      ],
      "metadata": {
        "id": "velJU0WXdbtr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jF-Eq7IDTAV7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "is_target_log = True\n",
        "import torchmetrics as tm\n",
        "metric_to_monitor = \"rmsle\"\n",
        "metric = tm.MeanSquaredError(squared=False) if is_target_log else tm.MeanSquaredLogError()\n",
        "\n",
        "Model = model.PytorchModel(target_encoder = dd_train.get_encoder_target(),\n",
        "                                            is_target_log = True,\n",
        "                                            optimizer = \"Adam\",\n",
        "                                            metric_to_monitor = metric_to_monitor,\n",
        "                                            numerical_input_size=dd_train.get_data_numerical().shape[1],\n",
        "                                            numerical_batch_normalization = True,\n",
        "                                            categorical_embedding_size=dd_train.get_data_categorical_embedding_sizes(),\n",
        "                                            categorical_embedding_dropout = 0.4,\n",
        "                                            text_as_embedding_bag = False,\n",
        "                                            text_as_embedding_bag_mode = \"mean\",\n",
        "                                            text_vocabulary_size = dd_train.get_text_vocabulary_size(),\n",
        "                                            text_embedding_dimension = 50,\n",
        "                                            text_bidirectional = True,\n",
        "                                            text_recurrent_hidden_size = 100,\n",
        "                                            text_recurrent_layers = 2,\n",
        "                                            text_rnn = \"GRU\",\n",
        "                                            char_vocabulary_size = dd_train.get_char_vocabulary_size(),\n",
        "                                            char_embedding_dimension = 40,\n",
        "                                            char_bidirectional = False,\n",
        "                                            char_recurrent_hidden_size = 50,\n",
        "                                            char_recurrent_layers = 1,\n",
        "                                            char_rnn = \"LSTM\",\n",
        "                                            linear_layer_skip_connections = (3, ([1024], [0.3])),\n",
        "                                            linear_layers = ([512], [0.2]),\n",
        "                                            linear_layer_normalization = \"BatchNorm1d\",\n",
        "                                            normalization_before_activation = True,\n",
        "                                            linear_layer_activation = nn.ReLU(inplace=True),\n",
        "                                            final_linear_layer=True,\n",
        "                                            final_normalization = False,\n",
        "                                            loss_function = nn.MSELoss(),\n",
        "                                            learning_rate = 0.001,\n",
        "                                            verbose = True\n",
        "                              )\n",
        "Model"
      ],
      "metadata": {
        "id": "V8zhFDWAv3OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63de69d-6595-4778-83c0-dcad10f69440"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"categorical_embedding_dropout\":   0.4\n",
            "\"categorical_embedding_size\":      [(12489, 315)]\n",
            "\"char_bidirectional\":              False\n",
            "\"char_embedding_dimension\":        40\n",
            "\"char_recurrent_hidden_size\":      50\n",
            "\"char_recurrent_layers\":           1\n",
            "\"char_rnn\":                        LSTM\n",
            "\"char_vocabulary_size\":            {'TITLE': 2333, 'DESCRIPTION': 4886, 'BULLET_POINTS': 3154}\n",
            "\"final_linear_layer\":              True\n",
            "\"final_normalization\":             False\n",
            "\"is_target_log\":                   True\n",
            "\"learning_rate\":                   0.001\n",
            "\"linear_layer_activation\":         ReLU(inplace=True)\n",
            "\"linear_layer_normalization\":      BatchNorm1d\n",
            "\"linear_layer_skip_connections\":   (3, ([1024], [0.3]))\n",
            "\"linear_layers\":                   ([512], [0.2])\n",
            "\"loss_function\":                   MSELoss()\n",
            "\"metric_to_monitor\":               rmsle\n",
            "\"normalization_before_activation\": True\n",
            "\"numerical_batch_normalization\":   True\n",
            "\"numerical_input_size\":            6\n",
            "\"optimizer\":                       Adam\n",
            "\"pretrained_hparams\":              False\n",
            "\"target_encoder\":                  PowerTransformer(method='box-cox')\n",
            "\"target_range\":                    None\n",
            "\"text_as_embedding_bag\":           False\n",
            "\"text_as_embedding_bag_mode\":      mean\n",
            "\"text_bidirectional\":              True\n",
            "\"text_embedding_dimension\":        50\n",
            "\"text_recurrent_hidden_size\":      100\n",
            "\"text_recurrent_layers\":           2\n",
            "\"text_rnn\":                        GRU\n",
            "\"text_vocabulary_size\":            {'TITLE': 768429, 'DESCRIPTION': 578497, 'BULLET_POINTS': 473100}\n",
            "\"verbose\":                         True\n",
            "processing text\n",
            "processing chars\n",
            "sizes detailed: [['cat: 315', 'num: 6', 'text: 600', 'char:150'], 'skip: 1024', 'lin: 512', 1]\n",
            "sizes: [1071, 1024, 512, 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PytorchModel(\n",
              "  (metric): MeanSquaredError()\n",
              "  (loss_function): MSELoss()\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(12489, 315, padding_idx=0)\n",
              "  )\n",
              "  (categorical_dropout): Dropout(p=0.4, inplace=False)\n",
              "  (batch_normalization_numerical): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (text_embeddings): ModuleList(\n",
              "    (0): TextRecurrentLayer(\n",
              "      (embedding): Embedding(768429, 50, padding_idx=0)\n",
              "      (rnn): GRU(50, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "    (1): TextRecurrentLayer(\n",
              "      (embedding): Embedding(578497, 50, padding_idx=0)\n",
              "      (rnn): GRU(50, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "    (2): TextRecurrentLayer(\n",
              "      (embedding): Embedding(473100, 50, padding_idx=0)\n",
              "      (rnn): GRU(50, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "  )\n",
              "  (char_embeddings): ModuleList(\n",
              "    (0): TextRecurrentLayer(\n",
              "      (embedding): Embedding(2333, 40, padding_idx=0)\n",
              "      (rnn): LSTM(40, 50, batch_first=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "    (1): TextRecurrentLayer(\n",
              "      (embedding): Embedding(4886, 40, padding_idx=0)\n",
              "      (rnn): LSTM(40, 50, batch_first=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "    (2): TextRecurrentLayer(\n",
              "      (embedding): Embedding(3154, 40, padding_idx=0)\n",
              "      (rnn): LSTM(40, 50, batch_first=True)\n",
              "      (last_time_step): LastTimeStep()\n",
              "    )\n",
              "  )\n",
              "  (linear_layers): Sequential(\n",
              "    (0): SkipDenseConnection(\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (skip_layers): ModuleList(\n",
              "        (0-1): 2 x LinearNormDropActivation(\n",
              "          (0): Linear(in_features=1071, out_features=1071, bias=False)\n",
              "          (1): BatchNorm1d(1071, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (2): LinearNormDropActivation(\n",
              "          (0): Linear(in_features=2142, out_features=1024, bias=False)\n",
              "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): LinearNormDropActivation(\n",
              "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Sequential(\n",
              "    (0): LinearNormDropActivation(\n",
              "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dump_path = '/content/drive/MyDrive/AmazonData/model'"
      ],
      "metadata": {
        "id": "SWoxDSIXw4QX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# early_stop_callback = EarlyStopping(min_delta=0.00, patience=5, mode=\"min\", verbose = True)\n",
        "\n",
        "checkpoint_filename = \"epoch{epoch:02d}-loss{loss:.2f}-val_loss{val_loss:.2f}\"\n",
        "model_checkpoint_callback = ModelCheckpoint(monitor = metric_to_monitor , filename=checkpoint_filename,\n",
        "                                            auto_insert_metric_name=False,\n",
        "                                            dirpath=model_dump_path,\n",
        "                                            save_weights_only = False,\n",
        "                                            verbose = True)\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "enable_model_summary = False\n",
        "\n",
        "print(f\"epochs: {epochs}\")\n",
        "\n",
        "#limit_train_batches=0.1\n",
        "trainer = pl.Trainer(precision= '16-mixed',\n",
        "                     accelerator = \"auto\",\n",
        "                     devices = 1,\n",
        "                     enable_checkpointing = True,\n",
        "                     check_val_every_n_epoch  = 1,\n",
        "                     max_epochs=epochs,\n",
        "                     enable_model_summary = enable_model_summary,\n",
        "                     default_root_dir = \"./\",\n",
        "                     enable_progress_bar = True,\n",
        "                     deterministic = False,\n",
        "                     callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmfjZtlQwr30",
        "outputId": "19449788-92b6-4edf-f8e0-d9a700b2ed32"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv['length_log'].dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUYVC1m6qWHJ",
        "outputId": "1d81919f-27fc-4750-e66d-9d749e6272cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "iyeSx_1iWLk0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(Model, train_dataloaders=train_loader , val_dataloaders  = validation_loader)"
      ],
      "metadata": {
        "id": "RaiLXzRAw3Ch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}